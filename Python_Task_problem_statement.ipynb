{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################                        Twitter Scrapper API                 ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "  \n",
    "# Fill the X's with the credentials obtained by  \n",
    "# following the above mentioned procedure. \n",
    "consumer_key = 'XXXXXXXXXXXXXX' \n",
    "consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "  \n",
    "# Function to extract tweets \n",
    "def get_tweets(username,path): \n",
    "          \n",
    "        # Authorization to consumer key and consumer secret \n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "  \n",
    "        # Access to user's access key and access secret \n",
    "        auth.set_access_token(access_key, access_secret) \n",
    "  \n",
    "        # Calling api \n",
    "        api = tweepy.API(auth)\n",
    "  \n",
    "        # 200 tweets to be extracted \n",
    "        number_of_tweets=2000\n",
    "        tweets = api.user_timeline(screen_name=username,count = number_of_tweets)\n",
    "  \n",
    "        # Empty Array Creation\n",
    "        media_for_tweets =[]\n",
    "        media_type =[]\n",
    "        extended_entities_media_for_tweets =[]\n",
    "        extended_entities_count_for_tweets =[]\n",
    "        photo_count_for_tweets=[]\n",
    "        dict_tweets = {'Text':[],'Date/Time_of_tweet':[],'Likes':[],'Retweets':[],'No_of_images_present':[]}\n",
    "        \n",
    "        # create array of tweet information: username,  \n",
    "        # tweet id, date/time, text \n",
    "        id_for_tweets = [tweet.id for tweet in tweets] #id file created\n",
    "        text_for_tweets = [tweet.text for tweet in tweets] # text file created\n",
    "        date_for_tweets = [tweet.created_at for tweet in tweets] # Date file created\n",
    "        likes_for_tweets = [tweet.favorite_count for tweet in tweets] # likes file created\n",
    "        retweet_for_tweets = [tweet.retweet_count for tweet in tweets] #retweet file created\n",
    "        \n",
    "        ####Creating the no of photo count file\n",
    "        #Checking for Extended_entities(We are using only extended entities because its more efficient )\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                extended_entities_media_for_tweets.append(tweet.extended_entities['media'])\n",
    "            except:\n",
    "                extended_entities_media_for_tweets.append([])\n",
    "        for media in extended_entities_media_for_tweets:\n",
    "            count =0\n",
    "            for i in range(10):\n",
    "                try:\n",
    "                    if((media[i]['type'])=='photo'):\n",
    "                        count = count+1\n",
    "                except:\n",
    "                    pass\n",
    "            extended_entities_count_for_tweets.append(count)\n",
    "        #Checking for normal entities\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "              media_for_tweets.append(tweet.entities['media'])\n",
    "            except:\n",
    "              media_for_tweets.append([])\n",
    "       \n",
    "        for media in media_for_tweets:\n",
    "            count =0\n",
    "            try:\n",
    "                if((media[0]['type'])=='photo'):\n",
    "                    count = count+1\n",
    "            except:\n",
    "                pass\n",
    "            photo_count_for_tweets.append(count)\n",
    "            \n",
    "        #Arranging them in dictionary\n",
    "        dict_tweets['Text'] = text_for_tweets\n",
    "        dict_tweets['Date/Time_of_tweet'] = date_for_tweets\n",
    "        dict_tweets['Likes'] = likes_for_tweets\n",
    "        dict_tweets['Retweets'] = retweet_for_tweets\n",
    "        dict_tweets['No_of_images_present'] = extended_entities_count_for_tweets\n",
    "        \n",
    "        #Dumping them in Json File\n",
    "        new_path = os.path.join(path,'tweet_tabular.json')\n",
    "        with open(new_path,\"w\") as file:\n",
    "            json.dump(dict_tweets,file)\n",
    "        \n",
    "        #Converting them in dataframe\n",
    "        tweet_dataframe = pd.DataFrame(dict_tweets)\n",
    "        return tweet_dataframe\n",
    "  \n",
    "  \n",
    "# Driver code \n",
    "if __name__ == '__main__': \n",
    "    #Specify the path to dump the files\n",
    "    path = '/'\n",
    "    # Here goes the twitter handle for the user \n",
    "    # whose tweets are to be extracted. \n",
    "    Tweet_dataframe = get_tweets(\"@midasIIITD\",path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
